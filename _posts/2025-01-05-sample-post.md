---
layout: single
title: "Model Distillation: Building High-Performance Small Language Models"
date: 2025-01-05
categories: [AI, Machine Learning]
tags: [Distillation, Arcee, Deep Learning, NLP, YouTube]
author: "Misagh Kordi"
header:
  overlay_image: /assets/images/sample-header.jpg
  overlay_filter: 0.5
  caption: "Photo credit: Unsplash"
  actions:
    - label: "Read More"
      url: "/about/"
excerpt: "Exploring model distillation techniques to create high-performance small language models."
---

## Model Distillation: Building High-Performance Small Language Models

This is a **YouTube video** explaining **model distillation**, an advanced technique to build high-performance small language models at a reasonable cost. 

It introduces two popular techniques for distillation:
1. **Logits Distillation**  
2. **Hidden States Distillation**  

The video explores how these methods work and how they're implemented in the **Arcee DistillKit** open-source library. Finally, it reviews two Arcee models built with distillation: **Arcee SuperNova 70B** and **Arcee SuperNova Medius 14B**.

---

### **Watch the Video:**

<div style="text-align: center;">
  <a href="https://www.youtube.com/watch?v=JE7SuP049mQ&t=183s" target="_blank">
    <img src="https://img.youtube.com/vi/JE7SuP049mQ/0.jpg" alt="Model Distillation Video" style="width: 60%; border-radius: 8px;">
  </a>
  <br>
  <a href="https://www.youtube.com/watch?v=JE7SuP049mQ&t=183s" target="_blank" style="font-size: 16px; font-weight: bold;">â–¶ Click to Play Video</a>
</div>

---

### **Tags:**

- [**Distillation**](/tags/#distillation)  
- [**Arcee**](/tags/#arcee)  
- [**Deep Learning**](/tags/#deep-learning)  
- [**NLP**](/tags/#nlp)  
- [**YouTube**](/tags/#youtube)  

---

### **References:**
- **Arcee DistillKit Documentation**  
- Research papers on **distillation techniques in NLP**
